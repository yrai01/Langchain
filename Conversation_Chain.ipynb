{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a92ed6",
   "metadata": {},
   "source": [
    "# Conversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a3a786",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d46297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-core langchain-community "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3c9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models.base import ChatOpenAI\n",
    "api_key = \"\"\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b84967",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conversation Buffer Memory\n",
    "\n",
    "ConversationBufferMemory usage is straightforward. It simply keeps the entire conversation in the buffer memory up to the allowed max limit (e.g. 4096 for gpt-3.5-turbo, 8192 for gpt-4). The main downside, however, is the cost. Each request is sending the aggregation to the API. Since the charge is based on token usages, the cost can quickly add up, especially if you are sending requests to an AI platform. Additionally, there can be added latency given the sheer size of the text that’s being passed back and forth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c473bb2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732e2f12",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conversation_buffer = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cac8dad",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/c100-122/opt/anaconda3/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi There!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi There!',\n",
       " 'history': '',\n",
       " 'response': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buffer(\"Hi There!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4483000b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi There!\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: What are some popular ones?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are some popular ones?',\n",
       " 'history': 'Human: Hi There!\\nAI: Hello! How can I assist you today?',\n",
       " 'response': 'Popular what? Can you please provide more context so I can better assist you?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buffer(\"What are some popular ones?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb0686",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conversation Buffer Window Memory\n",
    "Given that most conversations will not need context from several messages before, we also have the option of also using ConversationBufferWindowMemory. With this option we can simply control the buffer with a window of the last k messages.\n",
    "\n",
    "This enables us to control how much context is sent in the request, yet still maintain adequate recent conversational history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b1cdc87",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory.buffer_window import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d73f38",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conversation_buffer_window = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferWindowMemory(k=3),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "459deddb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi There!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi There!',\n",
       " 'history': '',\n",
       " 'response': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buffer_window(\"Hi There!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97fe3a25",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi There!\n",
      "AI: Hello! How can I assist you today?\n",
      "Human: What are some popular ones?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are some popular ones?',\n",
       " 'history': 'Human: Hi There!\\nAI: Hello! How can I assist you today?',\n",
       " 'response': 'Popular what? Can you please provide more context so I can better assist you?'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buffer_window(\"What are some popular ones?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b5520",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conversation Summary Memory\n",
    "ConversationSummaryMemory does not keep the entire history in memory like ConversationBufferMemory. Nor does it maintain a window. Rather, the ConversationSummaryMemory continually summarizes the conversation as it’s happening, and maintains it so we have context from the beginning of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e523ee8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory.summary import ConversationSummaryMemory\n",
    "from langchain_community.llms.openai import OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3210c4f0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conversation_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryMemory(llm=llm),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32a1e814",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What are some popular ones?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are some popular ones?',\n",
       " 'history': '',\n",
       " 'response': 'There are many popular AI assistants out there, such as Siri by Apple, Alexa by Amazon, Google Assistant, and Cortana by Microsoft. These assistants are widely used for tasks like setting reminders, answering questions, playing music, and controlling smart home devices. Each assistant has its own unique features and capabilities, making them popular choices for users looking to simplify their daily tasks.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Final question in the ongoing conversation.\n",
    "\"\"\"\n",
    "\n",
    "conversation_summary(\"What are some popular ones?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0d7d16",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conversation Summary Buffer Memory\n",
    "ConversationSummaryBufferMemory gives more nuanced control to maintain the most recent messages while summarizing the previous ones. There is the ability to flush into the summary based on a token count and keep the rest in the buffer. This could be helpful when it is important to have exact context of the last few messages, while still maintaining summary of older interactions within the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d285f1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory.summary_buffer import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48ecd602",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conversation_summary_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=2000),\n",
    "    verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f306a8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conversation Token Buffer Memory\n",
    "ConversationTokenBufferMemory simply maintains the buffer by the token count and does not maintain. There is no summarizing with this approach . In fact, it is similar to ConversationBufferWindowMemory, except instead of flushing based on the last number of messages (k), flushing is based on the number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee900415",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory.token_buffer import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70a9221c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conversation_token_buffer = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationTokenBufferMemory(llm=llm, max_token_limit=500, return_messages=True),\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
