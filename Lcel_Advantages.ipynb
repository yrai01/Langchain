{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a5b15e",
   "metadata": {},
   "source": [
    "## LCEL and Chain Interface\n",
    "\n",
    "LCEL (LangChain Expression Language) is a declarative method designed for succinctly describing chains.\n",
    "\n",
    "While LLMs can suffice in simple applications, complex applications often necessitate the chaining of LLMs with other components. LangChain furnishes two superior frameworks for this purpose:\n",
    "\n",
    "    Chain interface: The conventional method\n",
    "    LCEL: LangChain Expression Language\n",
    "\n",
    "For those constructing new applications, the use of LCEL is advocated. Notably, Chain can be integrated within LCEL, allowing for a hybrid utilization of both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6059762",
   "metadata": {},
   "source": [
    "## Advantages of LCEL\n",
    "\n",
    "The benefits of using LCEL include:\n",
    "\n",
    "    Asynchronous, Batch, and Streaming Support:\n",
    "    Chains crafted in LCEL inherently cater to both synchronous and asynchronous interfaces, as well as batch and streaming capabilities. This flexibility simplifies the process of starting with a synchronous prototype and later transitioning it to an asynchronous streaming interface.\n",
    "\n",
    "    Fallback in Chains:\n",
    "    With LCEL, any chain can readily incorporate fallbacks. This feature streamlines error management and fosters graceful handling.\n",
    "\n",
    "    Parallel Processing:\n",
    "    LCEL-composed chains inherently support parallel processing for all its components. Given that LLM applications frequently incorporate lengthy API calls, the importance of parallelism cannot be understated.\n",
    "\n",
    "    Seamless LangSmith Trace Integration:\n",
    "    An inherent trait of LCEL is the automatic logging of every step in LangSmith, ensuring optimum observability and debuggability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03189590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.0.321 langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff284dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3518d54",
   "metadata": {},
   "source": [
    "## Describe the chain with \"LCEL\".\n",
    "The chain that connects \"prompt template â†’ model\" is written as \" prompt | model \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f24604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompt_values import ChatPromptValue, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature = 0.9)\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell a joke about {topic}\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7482f169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='Tell a joke about {topic}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f7d8aa4cc10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f7d8aa60220>, temperature=0.9, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26c549",
   "metadata": {},
   "source": [
    "## Streaming responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a04c3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the car refuse to play hide and seek?\n",
      "\n",
      "Because it was always in the garage!"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"topic\": \"cars\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73195899",
   "metadata": {},
   "source": [
    "## LangChain Components and the Runnable Protocol\n",
    "\n",
    "LangChain components implement the \"Runnable\" protocol. This protocol provides a standard interface for defining custom chains and invoking them in a standardized manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260095d4",
   "metadata": {},
   "source": [
    "### Standard Interface\n",
    "\n",
    "The primary standard interfaces include:\n",
    "\n",
    "    stream: Stream back chunks of response.\n",
    "    invoke: Invoke chain with input.\n",
    "    batch: Invoke chain with a list of inputs.\n",
    "\n",
    "And there's a corresponding set of asynchronous methods:\n",
    "\n",
    "    astream: Asynchronously streams back a chunk of the response.\n",
    "    ainvoke: Asynchronously invokes the chain with an input.\n",
    "    abatch: Asynchronously invokes the chain with a list of inputs.\n",
    "    astream_log: Streams back intermediate steps in addition to the final response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4dfa43",
   "metadata": {},
   "source": [
    "### Input Type and Output Type\n",
    "Input Types\n",
    "\n",
    "Depending on the component, the types of input can be:\n",
    "\n",
    "    Prompt: Dictionary\n",
    "    Retriever: Single string\n",
    "    LLM, ChatModel: Single string, message list, PromptValue\n",
    "    Tool: Single string or dictionary (depends on the tool)\n",
    "    OutputParser: Output of LLM or ChatModel\n",
    "    \n",
    "Output Types\n",
    "\n",
    "Output types vary based on the component:\n",
    "\n",
    "    LLM: String\n",
    "    ChatModel: Chat message\n",
    "    Prompt: PromptValue\n",
    "    Retriever: List of documents\n",
    "    Tool: Depends on the tool\n",
    "    OutputParser: Depends on the parser   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68bd8af",
   "metadata": {},
   "source": [
    "### Checking Input and Output Schemas\n",
    "\n",
    "You can inspect the schemas for input and output types using:\n",
    "\n",
    "    input_schema: Input type schema (Pydantic)\n",
    "    output_schema: Output type schema (Pydantic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a8a6f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3688f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15208aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ChatOpenAIInput',\n",
       " 'anyOf': [{'type': 'string'},\n",
       "  {'$ref': '#/definitions/StringPromptValue'},\n",
       "  {'$ref': '#/definitions/ChatPromptValueConcrete'},\n",
       "  {'type': 'array',\n",
       "   'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "     {'$ref': '#/definitions/HumanMessage'},\n",
       "     {'$ref': '#/definitions/ChatMessage'},\n",
       "     {'$ref': '#/definitions/SystemMessage'},\n",
       "     {'$ref': '#/definitions/FunctionMessage'},\n",
       "     {'$ref': '#/definitions/ToolMessage'}]}}],\n",
       " 'definitions': {'StringPromptValue': {'title': 'StringPromptValue',\n",
       "   'description': 'String prompt value.',\n",
       "   'type': 'object',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'StringPromptValue',\n",
       "     'enum': ['StringPromptValue'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['text']},\n",
       "  'AIMessage': {'title': 'AIMessage',\n",
       "   'description': 'Message from an AI.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'HumanMessage': {'title': 'HumanMessage',\n",
       "   'description': 'Message from a human.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'ChatMessage': {'title': 'ChatMessage',\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'type': 'string'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role']},\n",
       "  'SystemMessage': {'title': 'SystemMessage',\n",
       "   'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['content']},\n",
       "  'FunctionMessage': {'title': 'FunctionMessage',\n",
       "   'description': 'Message for passing the result of executing a function back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'}},\n",
       "   'required': ['content', 'name']},\n",
       "  'ToolMessage': {'title': 'ToolMessage',\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'type': 'string'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id']},\n",
       "  'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',\n",
       "   'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n",
       "   'type': 'object',\n",
       "   'properties': {'messages': {'title': 'Messages',\n",
       "     'type': 'array',\n",
       "     'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "       {'$ref': '#/definitions/HumanMessage'},\n",
       "       {'$ref': '#/definitions/ChatMessage'},\n",
       "       {'$ref': '#/definitions/SystemMessage'},\n",
       "       {'$ref': '#/definitions/FunctionMessage'},\n",
       "       {'$ref': '#/definitions/ToolMessage'}]}},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ChatPromptValueConcrete',\n",
       "     'enum': ['ChatPromptValueConcrete'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['messages']}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc3a880f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ChatOpenAIOutput',\n",
       " 'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "  {'$ref': '#/definitions/HumanMessage'},\n",
       "  {'$ref': '#/definitions/ChatMessage'},\n",
       "  {'$ref': '#/definitions/SystemMessage'},\n",
       "  {'$ref': '#/definitions/FunctionMessage'},\n",
       "  {'$ref': '#/definitions/ToolMessage'}],\n",
       " 'definitions': {'AIMessage': {'title': 'AIMessage',\n",
       "   'description': 'Message from an AI.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'HumanMessage': {'title': 'HumanMessage',\n",
       "   'description': 'Message from a human.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'ChatMessage': {'title': 'ChatMessage',\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'type': 'string'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role']},\n",
       "  'SystemMessage': {'title': 'SystemMessage',\n",
       "   'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['content']},\n",
       "  'FunctionMessage': {'title': 'FunctionMessage',\n",
       "   'description': 'Message for passing the result of executing a function back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'}},\n",
       "   'required': ['content', 'name']},\n",
       "  'ToolMessage': {'title': 'ToolMessage',\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'type': 'string'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id']}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b8576e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the football coach go to the bank?\\n\\nTo get his quarterback!')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"Football\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e401d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Why did the man bring a ladder to his date? Because he heard love is in the air!'),\n",
       " AIMessage(content='Why did the romance writer break up with their partner? Because they realized they were just a chapter in their life, not the whole book!')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"Love\"}, {\"topic\": \"Romance\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6a13165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Why was the computer cold?\\n\\nBecause it left its Windows open!'),\n",
       " AIMessage(content=\"Why don't scientists trust atoms when they're traveling?\\n\\nBecause they make up everything!\")]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"Coding\"}, {\"topic\": \"Travelling\"}], config={\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3469411e",
   "metadata": {},
   "source": [
    "## Async Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daacf35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the satellite bring a camera to the party?   \n",
      "Because it wanted to take a groupie!"
     ]
    }
   ],
   "source": [
    "async for s in chain.astream({\"topic\": \"Satellites\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d6c8a",
   "metadata": {},
   "source": [
    "### Async Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0137c632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the tomato turn red?\n",
      "\n",
      "Because it saw the salad dressing!"
     ]
    }
   ],
   "source": [
    "async for s in chain.astream({\"topic\": \"Food\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd6b3a",
   "metadata": {},
   "source": [
    "### Async Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2c8148d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Why was the math book sad?\\n\\nBecause it had too many problems.'),\n",
       " AIMessage(content='Why did the cookie go to the doctor? Because it was feeling crumbly!')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.abatch([{\"topic\": \"Food\"}, {\"topic\": \"Sweets\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b225dab7",
   "metadata": {},
   "source": [
    "### Async Stream with intermediate steps\n",
    "\n",
    "Useful for displaying progress to the user, working with intermediate results, and debugging chains. You can stream all steps (default) or include or exclude steps by name, tags, or metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a433215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2519b",
   "metadata": {},
   "source": [
    "Execute the Retriever chain and output intermediate steps using astream_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daa83a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/c100-122/opt/anaconda3/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': '86d7c5bb-7113-485c-8ec9-e6ecea6bd86a',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'caf070d5-fb6e-4bae-aed0-1a12754bd29e',\n",
      "            'metadata': {},\n",
      "            'name': 'Docs',\n",
      "            'start_time': '2024-03-29T11:40:18.067+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n",
      "            'type': 'retriever'}})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs/final_output',\n",
      "  'value': {'documents': [Document(page_content='Sonu is the creator of AI Anytime Youtube Channel')]}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/Docs/end_time',\n",
      "  'value': '2024-03-29T11:40:18.434+00:00'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': ''})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Son'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Son'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'u'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Sonu'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''})\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "template = \"\"\"Please answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts([\"Sonu is the creator of AI Anytime Youtube Channel\"], embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever.with_config(run_name='Docs'), \"question\": RunnablePassthrough()}\n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async for chunk in retrieval_chain.astream_log(\"Who is the creator of AI Anytime?\", include_names=['Docs']):\n",
    "    print(\"-\"*40)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d66a6",
   "metadata": {},
   "source": [
    "## Parallel Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e9f05",
   "metadata": {},
   "source": [
    "\"RunnableParallel\" allows each element to run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7234bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import RunnableParallel\n",
    "chain1 = ChatPromptTemplate.from_template(\"Tell a joke about {topic}\") | model\n",
    "chain2 = ChatPromptTemplate.from_template(\"Write a short (2 line) poem about {topic}\") | model\n",
    "combined = RunnableParallel(joke=chain1, poem=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55bdbbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 ms, sys: 2.53 ms, total: 15.7 ms\n",
      "Wall time: 1.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Why did the AI break up with his girlfriend?\\nBecause she couldn't handle all his algorithms!\"),\n",
       " AIMessage(content=\"Why was the equal sign so humble?\\n\\nBecause he knew he wasn't less than or greater than anyone else!\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain1.batch([{\"topic\": \"AI\"}, {\"topic\": \"Math\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebede4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 ms, sys: 1.99 ms, total: 13.3 ms\n",
      "Wall time: 2.05 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='In the laboratory, truths are found,\\nThrough microscopes and chemicals profound.'),\n",
       " AIMessage(content='Juicy, sweet mango\\nGolden flesh that brings delight')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain2.batch([{\"topic\": \"Science\"}, {\"topic\": \"Mango\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39c94e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.2 ms, sys: 3.22 ms, total: 26.4 ms\n",
      "Wall time: 942 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'joke': AIMessage(content='Why did the robot go to school? To get a little byte of knowledge!'),\n",
       "  'poem': AIMessage(content='Artificial intelligence\\nMinds of silicon, thoughts immense.')},\n",
       " {'joke': AIMessage(content=\"Why are mountains so funny? Because they're hill-arious!\"),\n",
       "  'poem': AIMessage(content='Majestic peaks touch the sky,\\nSilent guardians watching from up high.')}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# combined\n",
    "combined.batch([{\"topic\": \"AI\"}, {\"topic\": \"Mountains\"}])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
