{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f702831",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e10ce",
   "metadata": {},
   "source": [
    "Recommended way to build language chain applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83bb12",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde92ae1",
   "metadata": {},
   "source": [
    "1. Parallel processing\n",
    "2. Streaming\n",
    "3. Batch Processing\n",
    "4. Fallback in chains: with LCEL, any kind of chains can readilly incorporate fallbacks.\n",
    "5. Integration with LangSmith as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb07029",
   "metadata": {},
   "source": [
    "### Basic example: prompt + model + output parser\n",
    "The most basic and common use case is chaining a prompt template and a model together. To see how this works, letâ€™s create a chain that takes a topic and generates a joke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83915cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.0.321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d259204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-core langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "110f1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d1e867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the ice cream break up with the cone? It couldn't handle the rocky road!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=OPENAI_API_KEY)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e46fd",
   "metadata": {},
   "source": [
    "The | symbol is similar to a unix pipe operator, which chains together the different components feeds the output from one component as input into the next component.\n",
    "\n",
    "In above given situation, we passed input to the prompt, then we passed output of this prompt template to the model, then model output is passed to the output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2c993",
   "metadata": {},
   "source": [
    "#### Prompt\n",
    "\n",
    "Prompt is a BasePromptTemplate, which means it takes in a dictionary of template variables and produces a PromptValue. A PromptValue is a wrapper around a completed prompt that can be passed to either an LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input). It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "650617bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ccfd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompt_values import ChatPromptValue, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fc1c7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ab87df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='tell me a short joke about ice cream')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c3005cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='tell me a short joke about ice cream')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[HumanMessage(content='tell me a short joke about ice cream')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81ecf644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: tell me a short joke about ice cream'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21941c8",
   "metadata": {},
   "source": [
    "#### Model\n",
    "Output of this prompt is now in model. Here we used ChatModel, so output will be BaseMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6446735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the ice cream go to therapy? Because it had too many sprinkles of self-doubt!', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 15, 'total_tokens': 37}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = model.invoke(prompt_value)\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f536cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy couldn't the ice cream go to the party? Because it was too cold to scoop.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\",api_key=OPENAI_API_KEY)\n",
    "llm.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9e3f6",
   "metadata": {},
   "source": [
    "#### Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2684998a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy? Because it had too many sprinkles of self-doubt!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f9686",
   "metadata": {},
   "source": [
    "#### Entire Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40eb7c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\"topic\": \"ice cream\"}\n",
    "\n",
    "prompt.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b9fbd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the ice cream truck break down?\\nIt had too many \"scoops\"!', response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 15, 'total_tokens': 34}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prompt | model).invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde05258",
   "metadata": {},
   "source": [
    "### RAG Search Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5798044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain docarray tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d18221ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 16:52:23.199025: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/c100-122/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.5\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2ea1c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01c6b827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='harrison worked at kensho'),\n",
       " Document(page_content='bears like to eat honey')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e37dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1517b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"where did harrison work?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95e08f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6dc20d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7f7c11e54520>),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f7c12c84af0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f7c12c7c640>, temperature=0.5, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72d9d1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harrison worked at Kensho.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(input=question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9770e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
